\section{Methods}

To investigate the functional distinction between propositional knowledge and sensory experience, we employed a dual-encoder Vision-Language Model (VLM) architecture. This setup allows us to operationalize the "Mind-Body" distinction computationally: the vision encoder represents the fixed biological hardware ($E_\phi$), and the language/fusion layers represent the plastic conceptual system ($P_\theta$).

\subsection{Model Architecture}

We utilized the \textbf{nanoVLM} architecture, chosen for its modular separation of vision and language components. The architecture consists of:

\begin{enumerate}
  \item \textbf{Sensory Encoder (Fixed Biology):} A SigLIP vision transformer. Crucially, the weights $\phi$ were \textbf{frozen} throughout the experiment. This constraint simulates the biological reality that the retina and primary visual cortex (V1) do not fundamentally restructure themselves based on semantic learning; they provide a fixed sensory distinct from high-level belief updating.
  \item \textbf{Conceptual Bridge (Plastic Mind):} A trainable multi-modal projection consisting of two Multi-Layer Perceptrons (MLPs). These MLPs map visual embeddings into the semantic space of the language model and represent the plastic, learned interface between sensory data and conceptual knowledge.
  \item \textbf{Propositional Decoder:} A transformer-based language model (SmolLM2-135M) representing Mary's store of ``textbook knowledge.''
\end{enumerate}

\begin{figure}[t]
  \centering
  \fbox{\parbox[c][3cm]{0.8\linewidth}{\centering Placeholder for Experimental Setup}}
  \caption{Experimental Setup.}
  \label{fig:experimental_setup}
\end{figure}

\begin{figure}[t]
  \centering
  \fbox{\parbox[c][3cm]{0.8\linewidth}{\centering Placeholder for MaryVLM Architecture}}
  \caption{MaryVLM Architecture.}
  \label{fig:maryvlm_architecture}
\end{figure}

\subsection{Stimuli and Data Partitions}

We utilized the \textbf{The Cauldron} dataset (HuggingFaceM4/the\_cauldron), a diverse collection of vision-language tasks for training. For evaluation, we employed the \textbf{MMStar} benchmark (Lin-Chen/MMStar), a comprehensive VQA test set designed to assess multimodal reasoning capabilities. We partitioned the evaluation data into two functional subsets:

\begin{itemize}
  \item \textbf{Internal Control (Achromatic):} Images presented in grayscale ($L, L, L$). These validate the agent’s \textbf{Conceptual Grounding}---its ability to identify shapes and textures (e.g., "This is a banana") based on its training.
  \item \textbf{External Release (Chromatic):} The \textit{same} images presented in RGB. Because the fusion layers have never encountered the chromatic format, these serve as the stimuli for the "Release" phase.
\end{itemize}

\subsection{Procedure}

\paragraph{Phase 1: Achromatic Acquisition ("The Room")}
To simulate Mary’s confinement, we trained the agent’s conceptual layers ($P_\theta$ and $D_\psi$) exclusively on grayscale images. An RGB-to-Luminance transformation $T(x)$ was applied to all training inputs. The model was optimized on Visual Question Answering (VQA) and Captioning tasks until it achieved asymptotic accuracy. During this phase, the model possessed "complete physical information" in the form of text descriptions (e.g., "Apples are red") but lacked the functional capacity to process the chromatic signal.

\paragraph{Phase 2: Chromatic Release (Paired-Stimulus Evaluation)}
To measure the "subjective shock" of the new format, we employed a \textbf{Paired-Stimulus Design}. We presented the model with identical images in both the Internal (Achromatic) and External (Chromatic) conditions. We hypothesized that if the \textit{Impenetrable Representation Hypothesis} is correct, the Chromatic condition should trigger a high-energy "surprise" signal, distinct from the low-energy state of recognizing the object in grayscale.

\subsection{Measures}

\paragraph{Novelty Detection (The "Wow" Signal)}
We operationalized the neural signature of novelty as \textbf{Mahalanobis Distance} ($D_M$). In biological systems, the Locus Coeruleus-Norepinephrine (LC-NE) system gates attention when predictions fail \citep{AstonJones2005}. Computationally, we modeled this as the distance of the incoming chromatic vector $z_c$ from the learned manifold of achromatic vectors $\mu_g$:

\begin{equation}
  S = D_M(z_c) - D_M(z_g)
\end{equation}

A significant positive $S$ indicates that "Redness" is not treated as just another feature, but as a \textbf{Violation of Expectation} (VoE) regarding the fundamental format of the input.

\paragraph{Subjective Specificity (The Inverted Spectrum)}
To test whether "qualia" are objective properties of the signal or subjective constructions of the agent, we trained two identical MaryVLM agents ($M_A$ and $M_B$) with different random seeds for the conceptual bridge. We extracted the latent centroid vectors for the concept "Red" from both agents. We utilized \textbf{Procrustes Analysis} to measure the alignment between their internal spaces. High functional equivalence (identical verbal outputs) combined with high Procrustes disparity (misaligned internal vectors) would support the hypothesis that subjective experience is \textit{structurally real} but \textit{implementation-dependent}.
