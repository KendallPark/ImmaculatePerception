\section{Prior Work}

Recent advancements at the intersection of Artificial Intelligence and the philosophy of mind have precipitated a "constructive approach" to consciousness, shifting the focus from metaphysical speculation to the engineering of systems that exhibit structural properties of subjective experience.

\subsection{The Constructive Approach and Bidirectional Influence}

The most direct theoretical antecedent to our framework is the work of \citet{Taniguchi2025}, who propose a "constructive approach" to the bidirectional influence between qualia structure and language emergence. They hypothesize that while perceptual experiences (upward organization) constrain the lexicon, language exerts a "downward constraint" on internal representations, forcing agents to align with a shared categorical structure. Our \textit{MaryVLM} model operationalizes this tension: the "subjective shock" we measure is the computational friction generated when a frozen upward constraint (biological vision) clashes with a plastic downward constraint (learned grayscale language).

\subsection{Generative AI and the "Mary's Room" Experiment}

The literature increasingly treats generative models as "philosophical laboratories". \citet{BelliniLeite2024} explicitly links Generative AI to the "Mary's Room" thought experiment, questioning whether systems trained on vast text datasets (propositional knowledge) can generate novel sensory instances without grounding. Furthermore, recent evaluations of Vision-Language Models (VLMs) like SmolVLM emphasize the utility of frozen vision encoders to prevent "catastrophic forgetting" of visual features. We adopt this architectural choice not just for robustness, but to enforce a strict informational encapsulation between sensation and concept, simulating the "read-only" nature of biological qualia.

\subsection{Structural Realism and Representational Alignment}

Finally, our approach draws on "Neurophenomenal Structuralism," which posits that while the intrinsic "content" of experience may be private, the relational structure of qualia spaces is mathematically formalizable. \citet{Sucholutsky2023} utilized Gromov-Wasserstein distance to demonstrate that neural networks can share "qualia structures" (e.g., color geometry) even if their individual activation axes are rotated or inverted. This supports the feasibility of our "Inverted Spectrum" analysis, suggesting that "redness" can be defined geometrically rather than chemically.
